{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd61e670",
   "metadata": {},
   "source": [
    "# MULAN inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08df5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = '/workspace/data/transformers_cache'\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mulan.dataset import ProteinDataset, data_collate_fn_dynamic\n",
    "from mulan.model import StructEsmForMaskedLM\n",
    "from mulan.model_utils import auto_detect_base_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e676ade2",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "To install repo run the command inside the folder MULAN:\n",
    "```bash\n",
    "git clone https://github.com/DFrolova/MULAN.git\n",
    "cd MULAN\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "In case of experimental structures, you firstly need to preprocess them with `pdbfixer` to restore missing atoms, remove water from the PDB file, etc.\n",
    "Firstly, you need to install `pdbfixer`. It should be installed in a separate environment for structure processing:\n",
    "```bash\n",
    "conda create -n pdb_processing python=3.7\n",
    "conda activate pdb_processing\n",
    "conda install pdbfixer=1.8.1 -c conda-forge\n",
    "conda install biopython=1.78\n",
    "```\n",
    "Then, you run the script, providing correct paths for `initial_structure_path` and `preprocessed_structure_path`.\n",
    "```bash\n",
    "python scripts/preprocess_experimental_structures.py\n",
    "```\n",
    "\n",
    "Also, you should download and put the foldseek binary file into the `mulan/bin` folder following the instructions provided in the [SaProt repo](https://github.com/westlake-repl/SaProt?tab=readme-ov-file#convert-protein-structure-into-structure-aware-sequence). \n",
    "Currently, they provide it [here](https://drive.google.com/file/d/1B_9t3n_nlj8Y3Kpc_mMjtMdY0OPYa7Re/view).\n",
    "Do not forget to add the rights for execution for foldseek (`chmod +x bin/foldseek`).\n",
    "If you do not need foldseek sequences (you use only MULAN based on ESM-2), you can pass `extract_foldseek_in_tokenizer=False` when initializing the `ProteinDataset`.\n",
    "Thus, you do not need to download and use the foldseek binary file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6dab4",
   "metadata": {},
   "source": [
    "### Define data paths and properties\n",
    "\n",
    "1) **Collect protein structures.**\n",
    "Put the protein structures (either `.pdb` or `.cif` files) you need to encode into the folder. \n",
    "The whole path to this folder is `protein_data_path`.\n",
    "If you need to extract certain chains from the structure (for experimental structures), you need to specify the chain name in the name of the file in the format: `{protein_id}_{chain}.pdb`.\n",
    "Othervise, the first chain would be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704ae299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "protein_data_path = '/workspace/data/docking/test_structures/PDB/tmp_preprocessed_structures/' # specify the path to the folder with pdb files you need to pass to the model\n",
    "saved_dataset_path = '/workspace/data/docking/test_structures/PDB_dataset/' # specify the path where to save the preprocessed dataset\n",
    "is_experimental_structure = True # flag for either AlphaFold structures or experimental ones\n",
    "\n",
    "# Model\n",
    "use_foldseek_sequences = False # True if use SaProt initialization for MULAN. Else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c7e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data \n",
    "# protein_data_path = <> # specify the path to the folder with pdb files you need to pass to the model\n",
    "# saved_dataset_path = <> # specify the path where to save the preprocessed dataset\n",
    "# is_experimental_structure = False # flag for either AlphaFold structures or experimental ones\n",
    "\n",
    "# # Model\n",
    "# use_foldseek_sequences = False # True if use SaProt initialization for MULAN. Else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18426c",
   "metadata": {},
   "source": [
    "### Initialize dataset and dataloader\n",
    "\n",
    "2) **Load the dataset and preprocess the data.**\n",
    "By default, the dataset would not aggeragate proteins into batches (1 portein = 1 batch). \n",
    "However, you can pass `use_sorted_batching=True` to the dataset (and still pass `batch_size=1` to the dataloader!) to aggregate proteins with similar lengths into the batch (maximim `batch_limit` tokens per batch) for a faster inference. \n",
    "Further steps of code suppose that each batch contains one protein.\n",
    "\n",
    "Note that initializing dataset requires firstly to preprocess and tokenize all provided protein structures.\n",
    "The results will be stored in the `saved_dataset_path` folder.\n",
    "Further, if you reuse this dataset, the preprocessing step is not required if you store the data inside the `saved_dataset_path`.\n",
    "\n",
    "During preprocessing, foldseek sequences are also extracted. \n",
    "This is done because SaProt-initialized MULAN uses Foldseek.\n",
    "If you use only ESM-2, you can pass argument `extract_foldseek_in_tokenizer=False` into the `ProteinDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2327457f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT ANGLES None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing data:   0%|                                                                                                      | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure <Structure id=>\n",
      "structure[0] <Model id=0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Preproc structural data: 1it [00:00, 2006.84it/s]\n",
      "1it [00:00, 3187.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein lengths before: 102 102\n",
      "Protein lengths after: 102 102\n",
      "Check for lengths 102 102 EEDTAILYPFTISGNDRNGNFTINFKGTPNSTNNGCIGYSYNGDWEKIEWEGSCDGNGNLVVEVPMSKIPAGVTSGEIQIWWHSGDLKMTDYKALEHHHHHH\n",
      "self.angles 1 torch.Size([102, 11])\n",
      "use_sorted_batching False\n",
      "1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preproc angles: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3858.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.angles 1 (1, 104, 7)\n",
      "self.plddts 1 (1, 102)\n",
      "[[100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      "  100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      "  100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      "  100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      "  100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      "  100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      "  100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      "  100. 100. 100. 100.]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = ProteinDataset(\n",
    "    protein_data_path=protein_data_path, \n",
    "    saved_dataset_path=saved_dataset_path,\n",
    "    use_foldseek_sequences=use_foldseek_sequences,\n",
    "    # batch_limit=5000, \n",
    "    # use_sorted_batching=True,\n",
    "    # extract_foldseek_in_tokenizer=False,\n",
    "    is_experimental_structure=is_experimental_structure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acbaed",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "Download the pre-trained model from [Zenodo](TODO) and put in into the checkpoint path, which you need to specify in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09a165b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "checkpoint_path = '/workspace/data/docking/lang_model/data/lora_train_results/mulan_small/' # path to the folder containing `config.json` and `model.safetensors` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93dea0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model\n",
    "# checkpoint_path = <> # path to the folder containing `config.json` and `model.safetensors` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca20c81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of StructEsmForMaskedLM were not initialized from the model checkpoint at /workspace/data/docking/lang_model/data/lora_train_results/mulan_small/ and are newly initialized: ['contact_head.regression.bias', 'contact_head.regression.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructEsmForMaskedLM(\n",
       "  (esm): StructEsmModel(\n",
       "    (embeddings): StructEsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
       "      (struct_embeddings): StructEmbeddings(\n",
       "        (MLP): Linear(in_features=7, out_features=320, bias=True)\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pooler): None\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): EsmLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=320, out_features=33, bias=False)\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StructEsmForMaskedLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b666c",
   "metadata": {},
   "source": [
    "### Prepare the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d50b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_tokenizer = auto_detect_base_tokenizer(model.config, use_foldseek_sequences)\n",
    "\n",
    "# Initialize dataloader\n",
    "def data_collator(x): \n",
    "    if use_foldseek_sequences:\n",
    "        one_letter_aas = esm_tokenizer.all_tokens[5:]\n",
    "    else: \n",
    "        one_letter_aas = dataset.tokenizer.one_letter_aas\n",
    "\n",
    "    return data_collate_fn_dynamic(x, \n",
    "        esm_tokenizer=esm_tokenizer,\n",
    "        nan_value=np.deg2rad(dataset.tokenizer.nan_fill_value),\n",
    "        mask_inputs=False,\n",
    "        all_amino_acids=one_letter_aas,\n",
    "        use_foldseek_sequences=use_foldseek_sequences)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2cdcf",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "Run inference to extract residue-level embeddings from the last layer of MULAN. \n",
    "Each batch contains one protein sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b91434ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5aot_A'] torch.Size([102, 320]) torch.Size([320])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (batch, batch_names) in enumerate(zip(dataloader, dataset.protein_names)):\n",
    "        struct_inputs = [struct_input.to(device) for struct_input in batch['struct_inputs']]\n",
    "        # extract embeddings for each batch (1 sequence per batch)\n",
    "        embeddings = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            struct_inputs=struct_inputs,\n",
    "            output_hidden_states=True\n",
    "        )['hidden_states'][-1]\n",
    "        embeddings = embeddings[0][1:-1] # residue-level embeddings for the case of 1 protein per batch\n",
    "        \n",
    "        # If you want to get protein-level embeddings, you shoud perform average pooling:\n",
    "        protein_embedding = embeddings.mean(dim=0)\n",
    "        print(batch_names, embeddings.shape, protein_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbd539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

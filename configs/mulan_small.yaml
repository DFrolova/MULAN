# Data paths
results_folder: '/workspace/data/docking/lang_model/data/lora_train_results/'
protein_data_path: None

saved_dataset_path: '/workspace/data/docking/lang_model/data/'
saved_dataset_path_AFDB: '/workspace/data/docking/lang_model/AFDB_data/'
use_foldseek_sequences: False

# model
esm_checkpoint: 'facebook/esm2_t6_8M_UR50D'
min_protein_length: 30 
max_protein_length: -1
num_struct_embeddings_layers: 1
use_struct_embeddings: True
predict_contacts: 'none'
predict_angles: False
mask_angle_inputs_with_plddt: True

use_sorted_batching: True
batch_limit: 12000 

# training
num_train_epochs: 100
train_batch_size: 1 
eval_batch_size: 1
logging_steps: 200
eval_steps: 5000
lr_scheduler_type: 'linear'
warmup_ratio: 0. 
learning_rate: 0.0001 

# trainer
use_AFDB: True
train_split: train
lr_decrease_ratio: 0.2

trained_adapter_name: None

# Data paths
results_folder: '/workspace/data/lang_model/data/lora_train_results/'
protein_data_path: None

saved_dataset_path: '/workspace/data/PDB_processed/'
saved_dataset_path_AFDB: '/workspace/data/AFDB_17M/'
split_ids_file: '/workspace/data/lang_model/data/train_val_test_split_small.json'
dataset_type: 'new_dataset_ext'
use_foldseek_sequences: True


# model
esm_checkpoint: 'westlake-repl/SaProt_35M_AF2'
base_checkpoint: None 
min_protein_length: 30 
max_protein_length: -1 
num_struct_embeddings_layers: 1
use_struct_embeddings: True 
predict_contacts: 'none'
predict_angles: False
mask_angle_inputs_with_plddt: True

use_sorted_batching: True
batch_limit: 32000 

# training
num_train_epochs: 50 
train_batch_size: 1 
eval_batch_size: 1
logging_steps: 400
eval_steps: 5000
lr_scheduler_type: 'linear' 
warmup_ratio: 0.001
learning_rate: 0.00001

# trainer
use_AFDB: True
train_split: train
lr_decrease_ratio: 0.2 

trained_adapter_name: None

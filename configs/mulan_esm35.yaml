# Data paths
results_folder: '/workspace/data/lang_model/data/lora_train_results/'
protein_data_path: None

saved_dataset_path: '/workspace/data/PDB_processed/'
saved_dataset_path_AFDB: '/workspace/data/AFDB_17M/'
use_foldseek_sequences: False


# model
esm_checkpoint: 'facebook/esm2_t12_35M_UR50D'
min_protein_length: 30 
max_protein_length: -1 
num_struct_embeddings_layers: 1
use_struct_embeddings: True 
predict_contacts: 'none'
predict_angles: False
mask_angle_inputs_with_plddt: True

use_sorted_batching: True
batch_limit: 32000 

# training
num_train_epochs: 50 
train_batch_size: 1 
eval_batch_size: 1
logging_steps: 400
eval_steps: 5000
lr_scheduler_type: 'linear' 
warmup_ratio: 0.001 
learning_rate: 0.00005 

# trainer
use_AFDB: True
train_split: train
lr_decrease_ratio: 0.2 

trained_adapter_name: None
